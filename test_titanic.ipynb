{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ICoNet as iconet\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass        Age  SibSp  Parch     Fare  \\\n",
       "0              1         0       3  22.000000      1      0   7.2500   \n",
       "1              2         1       1  38.000000      1      0  71.2833   \n",
       "2              3         1       3  26.000000      0      0   7.9250   \n",
       "3              4         1       1  35.000000      1      0  53.1000   \n",
       "4              5         0       3  35.000000      0      0   8.0500   \n",
       "..           ...       ...     ...        ...    ...    ...      ...   \n",
       "886          887         0       2  27.000000      0      0  13.0000   \n",
       "887          888         1       1  19.000000      0      0  30.0000   \n",
       "888          889         0       3  29.699118      1      2  23.4500   \n",
       "889          890         1       1  26.000000      0      0  30.0000   \n",
       "890          891         0       3  32.000000      0      0   7.7500   \n",
       "\n",
       "     Sex_female  Embarked_C  Embarked_Q  Embarked_S  \n",
       "0             0           0           0           1  \n",
       "1             1           1           0           0  \n",
       "2             1           0           0           1  \n",
       "3             1           0           0           1  \n",
       "4             0           0           0           1  \n",
       "..          ...         ...         ...         ...  \n",
       "886           0           0           0           1  \n",
       "887           1           0           0           1  \n",
       "888           1           0           0           1  \n",
       "889           0           1           0           0  \n",
       "890           0           0           1           0  \n",
       "\n",
       "[891 rows x 11 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "\n",
    "\n",
    "# Eliminate textual option\n",
    "df = df.drop(columns=['Name', 'Ticket', 'Cabin'])\n",
    "\n",
    "# One Hot Encoding\n",
    "df = pd.get_dummies(df).drop(columns=('Sex_male'))\n",
    "\n",
    "# Nan Values as mean\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "y = df['Survived']\n",
    "X = df.drop(columns='Survived')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 50)                550       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,801\n",
      "Trainable params: 10,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from multi_layer_perceptron import MLP_regressor\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "number_of_features = X.shape[1]\n",
    "\n",
    "model = MLP_regressor(number_of_features)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='mean_absolute_error',\n",
    "              optimizer=Adam())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "5/5 [==============================] - 2s 58ms/step - loss: 0.4425 - val_loss: 0.4191\n",
      "Epoch 2/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3830 - val_loss: 0.4105\n",
      "Epoch 3/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3758 - val_loss: 0.4085\n",
      "Epoch 4/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3734 - val_loss: 0.4078\n",
      "Epoch 5/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3735 - val_loss: 0.4076\n",
      "Epoch 6/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3711 - val_loss: 0.4077\n",
      "Epoch 7/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3710 - val_loss: 0.4079\n",
      "Epoch 8/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3713 - val_loss: 0.4082\n",
      "Epoch 9/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3692 - val_loss: 0.4084\n",
      "Epoch 10/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3698 - val_loss: 0.4088\n",
      "Epoch 11/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3682 - val_loss: 0.4090\n",
      "Epoch 12/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3690 - val_loss: 0.4092\n",
      "Epoch 13/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3672 - val_loss: 0.4090\n",
      "Epoch 14/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3690 - val_loss: 0.4091\n",
      "Epoch 15/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3659 - val_loss: 0.4095\n",
      "Epoch 16/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3643 - val_loss: 0.4103\n",
      "Epoch 17/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3678 - val_loss: 0.4101\n",
      "Epoch 18/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3651 - val_loss: 0.4093\n",
      "Epoch 19/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3635 - val_loss: 0.4122\n",
      "Epoch 20/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3679 - val_loss: 0.4138\n",
      "Epoch 21/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3656 - val_loss: 0.4138\n",
      "Epoch 22/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3660 - val_loss: 0.4143\n",
      "Epoch 23/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3623 - val_loss: 0.4111\n",
      "Epoch 24/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3611 - val_loss: 0.4113\n",
      "Epoch 25/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3649 - val_loss: 0.4112\n",
      "Epoch 26/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3627 - val_loss: 0.4123\n",
      "Epoch 27/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3592 - val_loss: 0.4133\n",
      "Epoch 28/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3599 - val_loss: 0.4131\n",
      "Epoch 29/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3594 - val_loss: 0.4117\n",
      "Epoch 30/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3593 - val_loss: 0.4109\n",
      "Epoch 31/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3592 - val_loss: 0.4105\n",
      "Epoch 32/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3587 - val_loss: 0.4115\n",
      "Epoch 33/250\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3582 - val_loss: 0.4117\n",
      "Epoch 34/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3573 - val_loss: 0.4113\n",
      "Epoch 35/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3621 - val_loss: 0.4114\n",
      "Epoch 36/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3606 - val_loss: 0.4112\n",
      "Epoch 37/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3606 - val_loss: 0.4113\n",
      "Epoch 38/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3578 - val_loss: 0.4107\n",
      "Epoch 39/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3614 - val_loss: 0.4107\n",
      "Epoch 40/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3568 - val_loss: 0.4103\n",
      "Epoch 41/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3567 - val_loss: 0.4104\n",
      "Epoch 42/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3582 - val_loss: 0.4105\n",
      "Epoch 43/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3589 - val_loss: 0.4109\n",
      "Epoch 44/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3572 - val_loss: 0.4115\n",
      "Epoch 45/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3554 - val_loss: 0.4110\n",
      "Epoch 46/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3579 - val_loss: 0.4095\n",
      "Epoch 47/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3558 - val_loss: 0.4098\n",
      "Epoch 48/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3606 - val_loss: 0.4099\n",
      "Epoch 49/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3537 - val_loss: 0.4120\n",
      "Epoch 50/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3567 - val_loss: 0.4112\n",
      "Epoch 51/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3599 - val_loss: 0.4110\n",
      "Epoch 52/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3555 - val_loss: 0.4104\n",
      "Epoch 53/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3571 - val_loss: 0.4104\n",
      "Epoch 54/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3592 - val_loss: 0.4103\n",
      "Epoch 55/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3604 - val_loss: 0.4102\n",
      "Epoch 56/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3599 - val_loss: 0.4103\n",
      "Epoch 57/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3588 - val_loss: 0.4103\n",
      "Epoch 58/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3560 - val_loss: 0.4104\n",
      "Epoch 59/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3569 - val_loss: 0.4110\n",
      "Epoch 60/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3554 - val_loss: 0.4120\n",
      "Epoch 61/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3599 - val_loss: 0.4113\n",
      "Epoch 62/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3583 - val_loss: 0.4103\n",
      "Epoch 63/250\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3586 - val_loss: 0.4102\n",
      "Epoch 64/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3590 - val_loss: 0.4102\n",
      "Epoch 65/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3577 - val_loss: 0.4103\n",
      "Epoch 66/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3583 - val_loss: 0.4103\n",
      "Epoch 67/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3540 - val_loss: 0.4112\n",
      "Epoch 68/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3529 - val_loss: 0.4108\n",
      "Epoch 69/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3553 - val_loss: 0.4104\n",
      "Epoch 70/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3574 - val_loss: 0.4102\n",
      "Epoch 71/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3575 - val_loss: 0.4102\n",
      "Epoch 72/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3570 - val_loss: 0.4104\n",
      "Epoch 73/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3570 - val_loss: 0.4115\n",
      "Epoch 74/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3533 - val_loss: 0.4105\n",
      "Epoch 75/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3588 - val_loss: 0.4104\n",
      "Epoch 76/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3554 - val_loss: 0.4108\n",
      "Epoch 77/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3549 - val_loss: 0.4118\n",
      "Epoch 78/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3557 - val_loss: 0.4102\n",
      "Epoch 79/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3553 - val_loss: 0.4102\n",
      "Epoch 80/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3532 - val_loss: 0.4102\n",
      "Epoch 81/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3577 - val_loss: 0.4101\n",
      "Epoch 82/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3525 - val_loss: 0.4070\n",
      "Epoch 83/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3587 - val_loss: 0.4070\n",
      "Epoch 84/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3546 - val_loss: 0.4089\n",
      "Epoch 85/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3544 - val_loss: 0.4072\n",
      "Epoch 86/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3556 - val_loss: 0.4070\n",
      "Epoch 87/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3550 - val_loss: 0.4072\n",
      "Epoch 88/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3554 - val_loss: 0.4075\n",
      "Epoch 89/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3602 - val_loss: 0.4100\n",
      "Epoch 90/250\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3525 - val_loss: 0.4102\n",
      "Epoch 91/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3565 - val_loss: 0.4102\n",
      "Epoch 92/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3562 - val_loss: 0.4101\n",
      "Epoch 93/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3553 - val_loss: 0.4085\n",
      "Epoch 94/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3551 - val_loss: 0.4095\n",
      "Epoch 95/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3532 - val_loss: 0.4102\n",
      "Epoch 96/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3567 - val_loss: 0.4101\n",
      "Epoch 97/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3550 - val_loss: 0.4079\n",
      "Epoch 98/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3540 - val_loss: 0.4118\n",
      "Epoch 99/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3601 - val_loss: 0.4086\n",
      "Epoch 100/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3541 - val_loss: 0.4102\n",
      "Epoch 101/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3552 - val_loss: 0.4102\n",
      "Epoch 102/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3565 - val_loss: 0.4102\n",
      "Epoch 103/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3537 - val_loss: 0.4102\n",
      "Epoch 104/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3549 - val_loss: 0.4091\n",
      "Epoch 105/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3606 - val_loss: 0.4160\n",
      "Epoch 106/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3588 - val_loss: 0.4150\n",
      "Epoch 107/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3561 - val_loss: 0.4128\n",
      "Epoch 108/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3558 - val_loss: 0.4132\n",
      "Epoch 109/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3558 - val_loss: 0.4133\n",
      "Epoch 110/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3526 - val_loss: 0.4107\n",
      "Epoch 111/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3582 - val_loss: 0.4102\n",
      "Epoch 112/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3567 - val_loss: 0.4102\n",
      "Epoch 113/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3558 - val_loss: 0.4114\n",
      "Epoch 114/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3548 - val_loss: 0.4118\n",
      "Epoch 115/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3574 - val_loss: 0.4107\n",
      "Epoch 116/250\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3542 - val_loss: 0.4103\n",
      "Epoch 117/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3533 - val_loss: 0.4102\n",
      "Epoch 118/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3541 - val_loss: 0.4102\n",
      "Epoch 119/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3541 - val_loss: 0.4115\n",
      "Epoch 120/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3587 - val_loss: 0.4132\n",
      "Epoch 121/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3595 - val_loss: 0.4113\n",
      "Epoch 122/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3575 - val_loss: 0.4114\n",
      "Epoch 123/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3550 - val_loss: 0.4104\n",
      "Epoch 124/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3558 - val_loss: 0.4103\n",
      "Epoch 125/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3574 - val_loss: 0.4109\n",
      "Epoch 126/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3542 - val_loss: 0.4129\n",
      "Epoch 127/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3563 - val_loss: 0.4128\n",
      "Epoch 128/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3550 - val_loss: 0.4119\n",
      "Epoch 129/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3540 - val_loss: 0.4106\n",
      "Epoch 130/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3521 - val_loss: 0.4102\n",
      "Epoch 131/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3536 - val_loss: 0.4102\n",
      "Epoch 132/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3541 - val_loss: 0.4102\n",
      "Epoch 133/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3530 - val_loss: 0.4102\n",
      "Epoch 134/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3562 - val_loss: 0.4102\n",
      "Epoch 135/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3577 - val_loss: 0.4102\n",
      "Epoch 136/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3558 - val_loss: 0.4102\n",
      "Epoch 137/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3560 - val_loss: 0.4102\n",
      "Epoch 138/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3585 - val_loss: 0.4105\n",
      "Epoch 139/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3534 - val_loss: 0.4110\n",
      "Epoch 140/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3570 - val_loss: 0.4106\n",
      "Epoch 141/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3549 - val_loss: 0.4102\n",
      "Epoch 142/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3570 - val_loss: 0.4102\n",
      "Epoch 143/250\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3529 - val_loss: 0.4102\n",
      "Epoch 144/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3535 - val_loss: 0.4102\n",
      "Epoch 145/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3526 - val_loss: 0.4102\n",
      "Epoch 146/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3548 - val_loss: 0.4102\n",
      "Epoch 147/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3522 - val_loss: 0.4102\n",
      "Epoch 148/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3553 - val_loss: 0.4102\n",
      "Epoch 149/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3516 - val_loss: 0.4102\n",
      "Epoch 150/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3508 - val_loss: 0.4102\n",
      "Epoch 151/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3547 - val_loss: 0.4102\n",
      "Epoch 152/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3583 - val_loss: 0.4099\n",
      "Epoch 153/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3574 - val_loss: 0.4077\n",
      "Epoch 154/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3548 - val_loss: 0.4092\n",
      "Epoch 155/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3570 - val_loss: 0.4101\n",
      "Epoch 156/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3524 - val_loss: 0.4102\n",
      "Epoch 157/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3544 - val_loss: 0.4102\n",
      "Epoch 158/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3530 - val_loss: 0.4098\n",
      "Epoch 159/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3549 - val_loss: 0.4089\n",
      "Epoch 160/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3573 - val_loss: 0.4096\n",
      "Epoch 161/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3540 - val_loss: 0.4102\n",
      "Epoch 162/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3540 - val_loss: 0.4100\n",
      "Epoch 163/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3537 - val_loss: 0.4088\n",
      "Epoch 164/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3540 - val_loss: 0.4062\n",
      "Epoch 165/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3513 - val_loss: 0.4020\n",
      "Epoch 166/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3513 - val_loss: 0.4036\n",
      "Epoch 167/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3477 - val_loss: 0.4010\n",
      "Epoch 168/250\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3374 - val_loss: 0.3742\n",
      "Epoch 169/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3532 - val_loss: 0.3556\n",
      "Epoch 170/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3363 - val_loss: 0.4064\n",
      "Epoch 171/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3562 - val_loss: 0.4101\n",
      "Epoch 172/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3558 - val_loss: 0.4102\n",
      "Epoch 173/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3622 - val_loss: 0.4102\n",
      "Epoch 174/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3575 - val_loss: 0.4102\n",
      "Epoch 175/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3570 - val_loss: 0.4102\n",
      "Epoch 176/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3524 - val_loss: 0.4102\n",
      "Epoch 177/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3540 - val_loss: 0.4101\n",
      "Epoch 178/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3515 - val_loss: 0.4097\n",
      "Epoch 179/250\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3561 - val_loss: 0.4076\n",
      "Epoch 180/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3523 - val_loss: 0.4046\n",
      "Epoch 181/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3514 - val_loss: 0.4038\n",
      "Epoch 182/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3587 - val_loss: 0.4045\n",
      "Epoch 183/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3508 - val_loss: 0.4093\n",
      "Epoch 184/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3526 - val_loss: 0.4057\n",
      "Epoch 185/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3458 - val_loss: 0.4034\n",
      "Epoch 186/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3502 - val_loss: 0.4034\n",
      "Epoch 187/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3481 - val_loss: 0.4034\n",
      "Epoch 188/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3475 - val_loss: 0.4039\n",
      "Epoch 189/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3504 - val_loss: 0.4003\n",
      "Epoch 190/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3447 - val_loss: 0.4032\n",
      "Epoch 191/250\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3470 - val_loss: 0.4034\n",
      "Epoch 192/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3450 - val_loss: 0.4034\n",
      "Epoch 193/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3471 - val_loss: 0.4034\n",
      "Epoch 194/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3457 - val_loss: 0.4068\n",
      "Epoch 195/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3446 - val_loss: 0.4068\n",
      "Epoch 196/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3431 - val_loss: 0.4068\n",
      "Epoch 197/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3456 - val_loss: 0.4068\n",
      "Epoch 198/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3505 - val_loss: 0.4068\n",
      "Epoch 199/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3487 - val_loss: 0.4068\n",
      "Epoch 200/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3463 - val_loss: 0.4068\n",
      "Epoch 201/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3485 - val_loss: 0.4033\n",
      "Epoch 202/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3453 - val_loss: 0.4068\n",
      "Epoch 203/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3460 - val_loss: 0.4066\n",
      "Epoch 204/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3474 - val_loss: 0.4071\n",
      "Epoch 205/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3474 - val_loss: 0.4068\n",
      "Epoch 206/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3495 - val_loss: 0.4067\n",
      "Epoch 207/250\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.3509 - val_loss: 0.4004\n",
      "Epoch 208/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3516 - val_loss: 0.4024\n",
      "Epoch 209/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3454 - val_loss: 0.4053\n",
      "Epoch 210/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3455 - val_loss: 0.4035\n",
      "Epoch 211/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3438 - val_loss: 0.4041\n",
      "Epoch 212/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3444 - val_loss: 0.4068\n",
      "Epoch 213/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3494 - val_loss: 0.4068\n",
      "Epoch 214/250\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3463 - val_loss: 0.4057\n",
      "Epoch 215/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3469 - val_loss: 0.4057\n",
      "Epoch 216/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3474 - val_loss: 0.4066\n",
      "Epoch 217/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3542 - val_loss: 0.4034\n",
      "Epoch 218/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3512 - val_loss: 0.4034\n",
      "Epoch 219/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3437 - val_loss: 0.4067\n",
      "Epoch 220/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3495 - val_loss: 0.4039\n",
      "Epoch 221/250\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3475 - val_loss: 0.4035\n",
      "Epoch 222/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3443 - val_loss: 0.4066\n",
      "Epoch 223/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3458 - val_loss: 0.4068\n",
      "Epoch 224/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3457 - val_loss: 0.4068\n",
      "Epoch 225/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3490 - val_loss: 0.4068\n",
      "Epoch 226/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3497 - val_loss: 0.4068\n",
      "Epoch 227/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3459 - val_loss: 0.4039\n",
      "Epoch 228/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3519 - val_loss: 0.4037\n",
      "Epoch 229/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3523 - val_loss: 0.4054\n",
      "Epoch 230/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3492 - val_loss: 0.4040\n",
      "Epoch 231/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3455 - val_loss: 0.4073\n",
      "Epoch 232/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3458 - val_loss: 0.4075\n",
      "Epoch 233/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3539 - val_loss: 0.4074\n",
      "Epoch 234/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3457 - val_loss: 0.4072\n",
      "Epoch 235/250\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3432 - val_loss: 0.4068\n",
      "Epoch 236/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3470 - val_loss: 0.4065\n",
      "Epoch 237/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3462 - val_loss: 0.4069\n",
      "Epoch 238/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3453 - val_loss: 0.4089\n",
      "Epoch 239/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3442 - val_loss: 0.4068\n",
      "Epoch 240/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3471 - val_loss: 0.4098\n",
      "Epoch 241/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3460 - val_loss: 0.4098\n",
      "Epoch 242/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3507 - val_loss: 0.4091\n",
      "Epoch 243/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3457 - val_loss: 0.4069\n",
      "Epoch 244/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3456 - val_loss: 0.4072\n",
      "Epoch 245/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3493 - val_loss: 0.4080\n",
      "Epoch 246/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3469 - val_loss: 0.4068\n",
      "Epoch 247/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3457 - val_loss: 0.4044\n",
      "Epoch 248/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3468 - val_loss: 0.4052\n",
      "Epoch 249/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3411 - val_loss: 0.4033\n",
      "Epoch 250/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3432 - val_loss: 0.4034\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 128\n",
    "epochs = 250\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle = True,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data = (X_test, y_test)\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABcIklEQVR4nO3deXgUVfY38G9nDyEJS8iChBBQNgMoQREQcACDqIzIqIiKoqLgiIjL/AYGkGVE3EB81YAwLoPDAK7oMKgEAQXiAhh2DMhiEMOqWSSQQHLfP87cVHUnna7OVp3w/TxPnuqu7q6+XWmok3PPvdehlFIgIiIi8mF+djeAiIiIyBMGLEREROTzGLAQERGRz2PAQkRERD6PAQsRERH5PAYsRERE5PMYsBAREZHPY8BCREREPi/A7gZUl5KSEvzyyy8IDw+Hw+GwuzlERERkgVIK+fn5aN68Ofz83OdR6k3A8ssvvyA+Pt7uZhAREVElHD58GC1atHD7eL0JWMLDwwHIB46IiLC5NURERGRFXl4e4uPjS6/j7tSbgEV3A0VERDBgISIiqmM8lXOw6JaIiIh8HgMWIiIi8nkMWIiIiMjn1ZsaFiIisl9xcTEKCwvtbgb5kICAAAQGBlZ5yhEGLEREVC3y8vLw448/Qilld1PIxzRs2BCtWrVCcHBwpY/BgIWIiKqsuLgYP/74I8LDwxEXF1fhBGB04VBKobCwEEeOHMGuXbuQlJSEoKCgSh2LAQsREVVZYWEhlFKIi4tDw4YN7W4O+ZCwsDAEBQUhMzMTq1evRr9+/RASEuL1cRgCExFRtWFmhcqjvxf79+/HqlWrUFxc7P0xqrtRREREROVp1KgRDh48iN9//93r1zJgISIiqkbXXHMNxo8fb/n5hw4dgsPhwNatW2usTQCwbt06OBwO5OTk1Oj7VCQwMBDnz5/H2bNnvX4ta1iIiOiC5GmY7T333IO3337b6+N++OGHCAwMtPz8+Ph4ZGdnIyoqyuv3upAwYCEiogtSdnZ26e1ly5bhqaeeQmZmZum+0NBQp+efO3fOUiDSpEkTr9rh7++P2NhYr15zIWKXkAdz5wLjxgE7dtjdEiIiqk6xsbGlP5GRkXA4HKX3z549i0aNGuHdd9/FNddcg5CQEPzrX//CqVOnMHz4cLRo0QINGjRAp06dsGTJEqfjunYJtWrVCs888wzuu+8+hIeHo2XLlliwYEHp465dQrrr5osvvkC3bt3QoEED9OzZ0ymYAoCnn34a0dHRCA8Px6hRozBhwgRcdtllXp2DDz74AJdeeimCg4PRqlUrzJ492+nx1NRUXHLJJQgJCUFMTAxuueWW0sfef/99dOrUCaGhoWjatCkGDBiA06dPe/X+3mDA4sGyZcArrwAHDtjdEiKiukMp4PRpe36qc966v/71rxg3bhz27NmDgQMH4uzZs0hOTsaKFSuwc+dOPPjggxgxYgS+/fbbCo8ze/ZsdOvWDRkZGfjzn/+Mhx56CD/88EOFr5k0aRJmz56NzZs3IyAgAPfdd1/pY4sXL8bMmTPx3HPPYcuWLWjZsiXmzZvn1WfbsmULbrvtNtx+++3YsWMHpk2bhilTppR2g23evBnjxo3DjBkzkJmZic8++wx9+vQBINmp4cOH47777sOePXuwbt06DB06tEYnDWSXkAd6hF4lRmAREV2wCgoAu6Zj+f13ICyseo41fvx4DB061Gnfk08+WXr7kUcewWeffYb33nsP3bt3d3uc66+/Hn/+858BSBD00ksvYd26dWjfvr3b18ycORN9+/YFAEyYMAE33HADzp49i5CQELzyyiu4//77ce+99wIAnnrqKaxatcqr0Tdz5sxB//79MWXKFABA27ZtsXv3brzwwgsYOXIksrKyEBYWhhtvvBHh4eFISEjA5ZdfDkAClvPnz2Po0KFISEgAAHTq1Mnye1cGMywe+PvLtqTE3nYQEVHt69atm9P94uJizJw5E507d0bTpk3RsGFDrFq1CllZWRUep3PnzqW3ddfT8ePHLb8mLi4OAEpfk5mZiSuvvNLp+a73PdmzZw969erltK9Xr17Yt28fiouLce211yIhIQGtW7fGiBEjsHjxYhQUFAAAunTpgv79+6NTp0649dZbsXDhQvz2229evb+3GLB4wAwLEZH3GjSQTIcdPw0aVN/nCHNJ1cyePRsvvfQS/u///g9r1qzB1q1bMXDgQBQVFVV4HNdiXYfDgRIPfwmbX6NHNJlf4zrKydvuGKVUhccIDw/H999/jyVLliAuLg5PPfUUunTpgpycHPj7+yMtLQ2ffvopOnbsiFdeeQXt2rXDwYMHvWqDNyoVsKSmpiIxMREhISFITk7G+vXrLb1u48aNCAgIqLAoaOnSpXA4HBgyZEhlmlbtmGEhIvKewyHdMnb8VHFR4AqtX78eN910E+666y506dIFrVu3xr59+2ruDd1o164dvvvuO6d9mzdv9uoYHTt2xIYNG5z2paeno23btvD/38UvICAAAwYMwPPPP4/t27fj0KFDWLNmDQAJmHr16oXp06cjIyMDQUFB+Oijj6rwqSrmdQ3LsmXLMH78eKSmpqJXr154/fXXMWjQIOzevRstW7Z0+7rc3Fzcfffd6N+/P44dO1buc3766Sc8+eST6N27t7fNqjHMsBARkXbxxRfjgw8+QHp6Oho3bow5c+bg6NGj6NChQ62245FHHsEDDzyAbt26oWfPnli2bBm2b9+O1q1bWz7GE088gSuuuAJ///vfMWzYMHz99dd49dVXkZqaCgBYsWIFDhw4gD59+qBx48ZYuXIlSkpK0K5dO3z77bf44osvkJKSgujoaHz77bc4ceJEjZ4HrzMsc+bMwf33349Ro0ahQ4cOmDt3LuLj4z1WJ48ePRp33HEHevToUe7jxcXFuPPOOzF9+nSvTnhNY4aFiIi0KVOmoGvXrhg4cCCuueYaxMbG2tIjcOedd2LixIl48skn0bVrVxw8eBAjR470alHBrl274t1338XSpUuRlJSEp556CjNmzMDIkSMByDT6H374Ifr164cOHTpg/vz5WLJkCS699FJERETgq6++wvXXX4+2bdti8uTJmD17NgYNGlRDnxhwKC86vYqKitCgQQO89957uPnmm0v3P/roo9i6dSu+/PLLcl/31ltvITU1FV9//TWefvppLF++vMwUxFOnTsX27dvx0UcfYeTIkcjJycHy5cvdtqWwsBCFhYWl9/Py8hAfH4/c3FxERERY/UgeXX898OmnwJtvAv8rxiYiIhcFBQXYs2cPOnTogAbVWURCll177bWIjY3FO++8Y3dTytDfj3379uHIkSO46667EBMTA0Cu35GRkR6v3151CZ08eRLFxcWlb6LFxMTg6NGj5b5m3759mDBhAtavX4+AgPLfbuPGjXjjjTe8Wkdh1qxZmD59uuXnVxYzLERE5GsKCgowf/58DBw4EP7+/liyZAlWr16NtLQ0u5tWYypVdFteVXF5azIUFxfjjjvuwPTp09G2bdtyj5Wfn4+77roLCxcu9GodhYkTJyI3N7f05/Dhw959CItYw0JERL7G4XBg5cqV6N27N5KTk/Gf//wHH3zwAQYMGGB302qMVxmWqKgo+Pv7l8mmHD9+vEzWBZBgZPPmzcjIyMDYsWMByJAspRQCAgKwatUqNGnSBIcOHcLgwYNLX6eHbQUEBCAzMxNt2rQpc+zg4GAEBwd70/xKYYaFiIh8TWhoKFavXm13M2qVVwFLUFAQkpOTkZaW5lTDkpaWhptuuqnM8yMiIrDDZRGe1NRUrFmzBu+//z4SExPh7+9f5jmTJ09Gfn4+Xn75ZcTHx3vTxGqnMywMWIiIiOzj9bDmxx9/HCNGjEC3bt3Qo0cPLFiwAFlZWRgzZgwA6ao5cuQIFi1aBD8/PyQlJTm9Pjo6GiEhIU77XZ/TqFGjcvfbgV1CRERE9vM6YBk2bBhOnTqFGTNmIDs7G0lJSVi5cmXpWgLZ2dkepyiuS9glRERkXU0ufkd1ly71qMr3o1KLH/75z38uXcTJlV7l0Z1p06Zh2rRpFT7H0zFqEzMsRESe6VGghYWFZaazJ9KLMnpawqAiXK3ZA2ZYiIg8CwwMRMOGDXHkyBEEBQXBz49L1ZFkVn7//XccOXIEOTk5KCkpKXdUsRUMWDxghoWIyDOHw4FWrVph165dyMzMtLs55GNycnJw7NgxnD17FgEBAQgNDfX6GAxYPGCGhYjImuDgYHTq1Alr167F3r17ER4eXmaVYrqwKKVw7tw5FBcXo7CwEDk5OejatWulug0ZsHjADAsRkXWBgYH4wx/+gPPnz+PAgQM4d+6c3U0iHxEYGIiuXbuif//+patBe4MBiwfMsBAReScwMBCDBg1CQUEBzp49a3dzyEeEhoYiNDSUNSw1hRkWIiLvORwOhIWFccQQVRuWcXvADAsREZH9GLB4wAwLERGR/RiweMAMCxERkf0YsHjADAsREZH9GLB4wAwLERGR/RiweKAzLAxYiIiI7MOAxQOdYWGXEBERkX0YsHjADAsREZH9GLB4wKJbIiIi+zFg8YBFt0RERPZjwOIBMyxERET2Y8DiATMsRERE9mPA4gEzLERERPZjwOIBMyxERET2Y8DiATMsRERE9mPA4gEzLERERPZjwOIBMyxERET2Y8DiATMsRERE9mPA4gGn5iciIrIfAxYPuPghERGR/RiweMAMCxERkf0YsHjADAsREZH9GLB4wAwLERGR/RiweMBhzURERPZjwOIBhzUTERHZjwGLB8ywEBER2Y8BiwfMsBAREdmPAYsHzLAQERHZjwGLB8ywEBER2Y8BiwfMsBAREdmPAYsHzLAQERHZjwGLB8ywEBER2Y8BiwfMsBAREdmPAYsHnJqfiIjIfgxYPODih0RERPZjwOIBMyxERET2Y8DiATMsRERE9mPA4gEzLERERPZjwOIBhzUTERHZjwGLBxzWTEREZD8GLB4ww0JERGQ/BiweMMNCRERkPwYsHjDDQkREZD8GLB4ww0JERGQ/BiweMMNCRERkPwYsHjDDQkREZL9KBSypqalITExESEgIkpOTsX79ekuv27hxIwICAnDZZZc57V+4cCF69+6Nxo0bo3HjxhgwYAC+++67yjSt2nHiOCIiIvt5HbAsW7YM48ePx6RJk5CRkYHevXtj0KBByMrKqvB1ubm5uPvuu9G/f/8yj61btw7Dhw/H2rVr8fXXX6Nly5ZISUnBkSNHvG1etePU/ERERPZzKKWUNy/o3r07unbtinnz5pXu69ChA4YMGYJZs2a5fd3tt9+OSy65BP7+/li+fDm2bt3q9rnFxcVo3LgxXn31Vdx9992W2pWXl4fIyEjk5uYiIiLC8ufx5PBhoGVLICgIKCystsMSERERrF+/vcqwFBUVYcuWLUhJSXHan5KSgvT0dLeve+utt7B//35MnTrV0vsUFBTg3LlzaNKkiTfNqxHMsBAREdkvwJsnnzx5EsXFxYiJiXHaHxMTg6NHj5b7mn379mHChAlYv349AgKsvd2ECRNw0UUXYcCAAW6fU1hYiEJTyiMvL8/Ssb3FGhYiIiL7Varo1uFwON1XSpXZB0jXzh133IHp06ejbdu2lo79/PPPY8mSJfjwww8REhLi9nmzZs1CZGRk6U98fLx3H8IinWFRSn6IiIio9nkVsERFRcHf379MNuX48eNlsi4AkJ+fj82bN2Ps2LEICAhAQEAAZsyYgW3btiEgIABr1qxxev6LL76IZ555BqtWrULnzp0rbMvEiRORm5tb+nP48GFvPoplfqYzxCwLERGRPbzqEgoKCkJycjLS0tJw8803l+5PS0vDTTfdVOb5ERER2LFjh9O+1NRUrFmzBu+//z4SExNL97/wwgt4+umn8fnnn6Nbt24e2xIcHIzg4GBvml8pOsMCSB2L+T4RERHVDq8CFgB4/PHHMWLECHTr1g09evTAggULkJWVhTFjxgCQzMeRI0ewaNEi+Pn5ISkpyen10dHRCAkJcdr//PPPY8qUKfj3v/+NVq1alWZwGjZsiIYNG1bl81UZMyxERET28zpgGTZsGE6dOoUZM2YgOzsbSUlJWLlyJRISEgAA2dnZHudkcZWamoqioiLccsstTvunTp2KadOmedvEamUOWDhSiIiIyB5ez8Piq2pqHpYzZ4AGDfR7AOHh1XZoIiKiC16NzMNyIWKGhYiIyH4MWDwwF9myhoWIiMgeDFg8YNEtERGR/RiweMAuISIiIvsxYLGA0/MTERHZiwGLBVwAkYiIyF4MWCxghoWIiMheDFgsYIaFiIjIXgxYLGCGhYiIyF4MWCxghoWIiMheDFgsYIaFiIjIXgxYLGCGhYiIyF4MWCxghoWIiMheDFgs0AELMyxERET2YMBige4SYoaFiIjIHgxYLGCGhYiIyF4MWCxghoWIiMheDFgsYNEtERGRvRiwWMBhzURERPZiwGIBMyxERET2YsBiATMsRERE9mLAYgEzLERERPZiwGIBMyxERET2YsBiATMsRERE9mLAYgEzLERERPZiwGIBMyxERET2YsBiATMsRERE9mLAYgEzLERERPZiwGIBMyxERET2YsBiATMsRERE9mLAYgEDFiIiInsxYLGAXUJERET2YsBiATMsRERE9mLAYgEzLERERPZiwGIBMyxERET2YsBiATMsRERE9mLAYgEzLERERPZiwGIBMyxERET2YsBiATMsRERE9mLAYgEzLERERPZiwGIBMyxERET2YsBiATMsRERE9mLAYgEzLERERPZiwGIBMyxERET2YsBiATMsRERE9mLAYgEDFiIiInsxYLGAXUJERET2YsBiATMsRERE9mLAYgEzLERERPZiwGIBMyxERET2YsBiATMsRERE9mLAYgEzLERERPZiwGIBMyxERET2qlTAkpqaisTERISEhCA5ORnr16+39LqNGzciICAAl112WZnHPvjgA3Ts2BHBwcHo2LEjPvroo8o0rUYww0JERGQvrwOWZcuWYfz48Zg0aRIyMjLQu3dvDBo0CFlZWRW+Ljc3F3fffTf69+9f5rGvv/4aw4YNw4gRI7Bt2zaMGDECt912G7799ltvm1cjmGEhIiKyl0Mppbx5Qffu3dG1a1fMmzevdF+HDh0wZMgQzJo1y+3rbr/9dlxyySXw9/fH8uXLsXXr1tLHhg0bhry8PHz66ael+6677jo0btwYS5YssdSuvLw8REZGIjc3FxEREd58JI+mTwemTQPGjAFMH5uIiIiqyOr126sMS1FREbZs2YKUlBSn/SkpKUhPT3f7urfeegv79+/H1KlTy33866+/LnPMgQMHVnjMwsJC5OXlOf3UFGZYiIiI7OVVwHLy5EkUFxcjJibGaX9MTAyOHj1a7mv27duHCRMmYPHixQgICCj3OUePHvXqmAAwa9YsREZGlv7Ex8d781G8whoWIiIie1Wq6NbhcDjdV0qV2QcAxcXFuOOOOzB9+nS0bdu2Wo6pTZw4Ebm5uaU/hw8f9uITeEdnWBiwEBER2aP8lIcbUVFR8Pf3L5P5OH78eJkMCQDk5+dj8+bNyMjIwNixYwEAJSUlUEohICAAq1atQr9+/RAbG2v5mFpwcDCCg4O9aX6l6QwLu4SIiIjs4VWGJSgoCMnJyUhLS3Pan5aWhp49e5Z5fkREBHbs2IGtW7eW/owZMwbt2rXD1q1b0b17dwBAjx49yhxz1apV5R7TDuwSIiIispdXGRYAePzxxzFixAh069YNPXr0wIIFC5CVlYUxY8YAkK6aI0eOYNGiRfDz80NSUpLT66OjoxESEuK0/9FHH0WfPn3w3HPP4aabbsLHH3+M1atXY8OGDVX8eNWDRbdERET28jpgGTZsGE6dOoUZM2YgOzsbSUlJWLlyJRISEgAA2dnZHudkcdWzZ08sXboUkydPxpQpU9CmTRssW7asNANjN2ZYiIiI7OX1PCy+qibnYXntNWDsWOCWW4D33qvWQxMREV3QamQelgsVMyxERET2YsBiAWtYiIiI7MWAxQJmWIiIiOzFgMUCZliIiIjsxYDFAmZYiIiI7MWAxQJmWIiIiOzFgMUCZliIiIjsxYDFAmZYiIiI7MWAxQJmWIiIiOzFgMUCnWFhwEJERGQPBiwW6AwLu4SIiIjswYDFAmZYiIiI7MWAxQJmWIiIiOzFgMUCFt0SERHZiwGLBRzWTEREZC8GLBYww0JERGQvBiwWMMNCRERkLwYsFjDDQkREZC8GLBYww0JERGQvBiwWMMNCRERkLwYsFjDDQkREZC8GLBYww0JERGQvBiwWcGp+IiIiezFgsYBT8xMREdmLAYsFzLAQERHZiwGLBcywEBER2YsBiwXMsBAREdmLAYsFzLAQERHZiwGLBRzWTEREZC8GLBZw4jgiIiJ7MWCxgBkWIiIiezFgsYAZFiIiInsxYLGAGRYiIiJ7MWCxgBkWIiIiezFgscDPdJaUsq8dREREFyoGLBboDAvALAsREZEdGLBYYM6wsI6FiIio9jFgscCcYWHAQkREVPsYsFhgzrCwS4iIiKj2MWCxgBkWIiIiezFgsYAZFiIiInsxYLGAGRYiIiJ7MWCxgBkWIiIiezFgscDhkB+AGRYiIiI7MGCxSGdZmGEhIiKqfQxYLOICiERERPZhwGJRQIBsz52ztx1EREQXIgYsFjVuLNvffrO3HURERBciBiwWRUXJ9uRJe9tBRER0IWLAYhEDFiIiIvswYLFIBywnTtjbDiIiogsRAxaLmjWTLTMsREREta9SAUtqaioSExMREhKC5ORkrF+/3u1zN2zYgF69eqFp06YIDQ1F+/bt8dJLL5V53ty5c9GuXTuEhoYiPj4ejz32GM6ePVuZ5tUIdgkRERHZJ8DbFyxbtgzjx49HamoqevXqhddffx2DBg3C7t270bJlyzLPDwsLw9ixY9G5c2eEhYVhw4YNGD16NMLCwvDggw8CABYvXowJEybgzTffRM+ePbF3716MHDkSAMoNbuzALiEiIiL7OJRSypsXdO/eHV27dsW8efNK93Xo0AFDhgzBrFmzLB1j6NChCAsLwzvvvAMAGDt2LPbs2YMvvvii9DlPPPEEvvvuuwqzN2Z5eXmIjIxEbm4uIiIivPhE1ixbBtx+O9C3L7BuXbUfnoiI6IJk9frtVZdQUVERtmzZgpSUFKf9KSkpSE9Pt3SMjIwMpKeno2/fvqX7rr76amzZsgXfffcdAODAgQNYuXIlbrjhBrfHKSwsRF5entNPTWKXEBERkX286hI6efIkiouLERMT47Q/JiYGR48erfC1LVq0wIkTJ3D+/HlMmzYNo0aNKn3s9ttvx4kTJ3D11VdDKYXz58/joYcewoQJE9web9asWZg+fbo3za8SBixERET2qVTRrUMvXfw/Sqky+1ytX78emzdvxvz58zF37lwsWbKk9LF169Zh5syZSE1Nxffff48PP/wQK1aswN///ne3x5s4cSJyc3NLfw4fPlyZj2KZOWDhekJERES1y6sMS1RUFPz9/ctkU44fP14m6+IqMTERANCpUyccO3YM06ZNw/DhwwEAU6ZMwYgRI0qzLp06dcLp06fx4IMPYtKkSfDzKxtXBQcHIzg42JvmV4kOWIqLgdxcY6p+IiIiqnleZViCgoKQnJyMtLQ0p/1paWno2bOn5eMopVBYWFh6v6CgoExQ4u/vD6UUvKwJrjHBwUB4uNxmtxAREVHt8npY8+OPP44RI0agW7du6NGjBxYsWICsrCyMGTMGgHTVHDlyBIsWLQIAvPbaa2jZsiXat28PQOZlefHFF/HII4+UHnPw4MGYM2cOLr/8cnTv3h0//vgjpkyZgj/+8Y/w9/evjs9ZLaKigPx8CVguucTu1hAREV04vA5Yhg0bhlOnTmHGjBnIzs5GUlISVq5ciYSEBABAdnY2srKySp9fUlKCiRMn4uDBgwgICECbNm3w7LPPYvTo0aXPmTx5MhwOByZPnowjR46gWbNmGDx4MGbOnFkNH7H6REUBBw9yLhYiIqLa5vU8LL6qpudhAYAbbgBWrgTeeAO4774aeQsiIqILSo3Mw3Kh49BmIiIiezBg8QIDFiIiInswYPEC1xMiIiKyBwMWLzRrJltmWIiIiGoXAxYvsEuIiIjIHgxYvMAuISIiInswYPFCbKxsjxwBzp2zty1EREQXEgYsXmjdGmjSBDh7Fvj+e7tbQ0REdOFgwOIFPz+gd2+5/eWX9raFiIjoQsKAxUt9+8r2q6/sbQcREdGFhAGLl/r0ke2GDUBxsb1tISIiulAwYPFSly5AeDiQmwts3253a4iIiC4MDFi8FBAAXH213H71VSA1FcjJsbVJRERE9R4DlkrQ3UJvvgk8/DDw9NP2toeIiKi+Y8BSCXfeCXTuLMOcAeCLL+xtDxERUX3HgKUS4uOBbduA9evl/rZtUtNCRERENYMBSxU0bw60aQMoBaSn290aIiKi+osBSxXpehbOy0JERFRzGLBUkZ75VncPERERUfVjwFJFOmD57jvgzBmgpARYuZJrDREREVWnALsbUNe1aQPExQHZ2cCIEcDhwxK8REbKvtBQu1tIRERU9zHDUkUOB3DzzXL7gw8kWAFk1BALcYmIiKoHMyzV4JVXgFtvBT79VAKY3buB//wHWL0a6N/f7tYRERHVfcywVAM/P+Caa4DnngOefRa45RbZv3q1rc0iIiKqNxiw1ACdVdmyRX7GjAF27rS3TURERHUZu4RqwEUXAR06AHv2yCiiM2eAgweBzz+3u2VERER1EzMsNURnWc6cke3q1cDRo0BxMVBYaF+7iIiI6iIGLDVk0CDZNmkCtG8v87MsXgwMHAhERwNZWcCRI5KNeeABe9tKRETk6xxKKWV3I6pDXl4eIiMjkZubi4iICLubA6WAf/8buOIK6QoaNw4IDATOnZPHZ82SYt2//hVo0ADIywP8/e1tMxERUW2zev1mDUsNcTiAO++U240aAY89ZgQrAPDhh8btggJg/36gbdtabSIREVGdwS6hWhAdbXQRDRkiwcymTfKjbdtm3D54EFixwvkY+fnAvfcC69bVdGuJiIh8DwOWWvL668CbbwJLlxrrD5mZA5aRI4HBg52DkyVLgLffBp58soYbSkRE5IMYsNSS5s0lQxIcDPzpT8b+Dh1kqwOWkhIj8/Lll8bz9u+XbUaGZFuIiIguJAxYbDB0qBTc+vkBTz0l+3TAkpVlDIU2dxkdPCjbkhKuUURERBceFt3aoEULYPlyqWXR3UOHDwO//irrEGmbNsloI4fDCFgA4KuvZHg0ERHRhYIBi00GDzZut2oFHDoEbN8us+Nqx49LINOypXPAsn59bbWSiIjIN7BLyAd06SLbbducAxZAsiz5+cCpU8a+b7+VLMvs2UBRUe21k4iIyC4MWHyADlg2bza6hJo2le2mTUZ2pUkTIDZWgpS+fWXE0Mcf1357iYiIahsDFh/Qr59sP/kE2LVLbt9xh2zNAUvr1mWHRHMVaCIiuhAwYPEBvXtLnUpenvz4+RkBy+bNxpDmVq1kiv+ePY2i2x9+cD7W8eNcXJGIiOofBiw+wM8PuOsu435iIpCcDISFSQCzdKmx/+qrgY0bgbFjZV9mpvG6nTtlMcXRo2uv7URERLWBAYuPGDHCuN2xoyyUePPNcl/Px5KYaDynfXvZZmbK3CwAsHYtcP488P77zusWERER1XUMWHxE+/aysjNgzH5rDmIA54ClVSsgKAg4e1YmmwOAvXtle/q086RzREREdR0DFh/y4otSgPvAA3K/f38ZFaSZA5aAAODii+W27hbSAQsg2RYiIqL6ggGLD+nTB/jiCyMQ8fc3im8BICHB+fm6W0gX3poDFq7qTERE9QkDFh83cqQELp06ASEhzo+ZA5YzZ4CffjIe27iRo4WIiKj+YMDi4zp1Ar77Dlixouxj7drJNjNThj4rBURGAtHREsB8913ttpWIiKimMGCpA7p2lXlaXJkzLLo7qF074Jpr5Da7hYiIqL5gwFKH6QxLdrYxKqhtW5lYDgAyMip/7E2bgJ9/rlr7iMh3nTkDjBkDfPaZ3S0hsoYBSx0WGWkU6M6fL9u2bYFLL5Xbel0ib+3fD/ToAfzhD8YcL0RUv3z+OfD668CkSXa3hMgaBix13KOPyjYnR7Zt28rEcwDw44+VK7zdsQMoLpbXl9et9PvvzqtHE1Hd88svstVrlRH5OgYsddz990uRrda2LRAXJ9mX4mJg3z7vj6knogOAf/7T+TGlgO7dgTZtgPz8yrWZiOx37Jhsf/tNlgAh8nWVClhSU1ORmJiIkJAQJCcnY/369W6fu2HDBvTq1QtNmzZFaGgo2rdvj5deeqnM83JycvDwww8jLi4OISEh6NChA1auXFmZ5l1QQkOBxx4z7l9yCeBwGFmWynQLmQOWDz6QjIp28qQcMzcX2LOncm0mIvvpgAVwnhKByFd5HbAsW7YM48ePx6RJk5CRkYHevXtj0KBByDJf5UzCwsIwduxYfPXVV9izZw8mT56MyZMnY8GCBaXPKSoqwrXXXotDhw7h/fffR2ZmJhYuXIiLLrqo8p/sAvLQQ8BllwG33AI0bCj7qitgOX1aJq8bM0ZSyObFFiv7n9ypU8Dy5d6td6QU62lqw+nTwN//7vwdoPrp+HHjNgMWqhOUl6688ko1ZswYp33t27dXEyZMsHyMm2++Wd11112l9+fNm6dat26tioqKvG1OqdzcXAVA5ebmVvoY9cns2UoBSt16q/ev7d5dXtu1q2z1z1/+otTChcb9F16oXNtGjZLX/+tf1p5//rxSvXop1bq1Utu3V+4966pjx5QqKam99/t//09+N4MG1d57kj169jT+Lb/6qt2toQuZ1eu3VxmWoqIibNmyBSkpKU77U1JSkJ6ebukYGRkZSE9PR9++fUv3ffLJJ+jRowcefvhhxMTEICkpCc888wyKi4vdHqewsBB5eXlOP2TQGZZdu+Sv5iNHrL9W/7X10kvAhAnA4MFyf/NmYxkA8/O8tXOnbH/80Xn/9u3lF/P+5z8yc++BA8DVV3ueX2b9ejlWXZeWBsTEAE89VbXj/PKL1DY995zn5+oCzNWrjUJuqp/YJUR1jVcBy8mTJ1FcXIyYmBin/TExMTh69GiFr23RogWCg4PRrVs3PPzwwxg1alTpYwcOHMD777+P4uJirFy5EpMnT8bs2bMxc+ZMt8ebNWsWIiMjS3/i4+O9+Sj1ng5Y9u6V7qKWLYHZs+XvqYoUFgL6V9mxIzBrlnQRAMCWLc51K4cOVa5t+j9Hc0r6hx+knUOGlH3+7NmyjYiQ4sA//UlqaMpz9KgsGtm3rwRqddmGDbKt6srba9dK8fXixZ6fq0eOnDsngWJtYXdf7WPAQnVNpYpuHQ6H032lVJl9rtavX4/Nmzdj/vz5mDt3LpYsWVL6WElJCaKjo7FgwQIkJyfj9ttvx6RJkzBv3jy3x5s4cSJyc3NLfw4fPlyZj1JvxcdLPcv585LJKCkBnnwSGDQIePllyVKUF2PqyeJCQ4GmTeV2x45AcLAEC+bsRmX+kysslInuAODECWP/xo0STG3Z4hxUffedXLgDA4GtW2V2319/BebOLf/4mZlysc3Jqd0Lbk3Q57eqQ8j179nD3xQAjN8NIAXXteHdd4GwMGMuIap5BQXOxfSV/eODqDZ5FbBERUXB39+/TDbl+PHjZbIurhITE9GpUyc88MADeOyxxzBt2rTSx+Li4tC2bVv4+/uX7uvQoQOOHj2KoqKico8XHByMiIgIpx8yOBxAhw5yu2FDYOJEWUTx88+B8eNlUri4OGDUKMB8inWxZcuWcgxAgoUuXeR2QYHx3J9+KpuxKSmRkUTumGfPNWdYtm2T7Zkzzn/56cBk+HAgMRGYPl3uz5kjgYsr85wS5ozCDz8A77wjQ73riuoOWE6c8FzorDMsgHxXzBe1mpCVBTzwAHD2LAOW2mT+NwYww0J1g1cBS1BQEJKTk5GWlua0Py0tDT31fPAWKKVQaJrRrFevXvjxxx9RYsoL7927F3FxcQgKCvKmiWRy++1A48bAkiXAM88A338v28GDZR4VhwN44w3g+uuNegX9H5fr2kXJycbtBg1km5fnXOdQUgLceqvMC+NupLv5P0ZzhsVcc6KDjqIiI0vy8MOyveUWoHNnee85c8oe3/yX4mefSe3OtGnymrvvNrqXqurpp4EWLbz/j76kBEhNNQK0ilR3wAI4B4mulDICloYNJYj49NOqvXdFSkpkNXJdfrZtmz3LQXjqJq2PdMASHm7cP3vW++Ns2CD1TkS1wttq3qVLl6rAwED1xhtvqN27d6vx48ersLAwdejQIaWUUhMmTFAjRowoff6rr76qPvnkE7V37161d+9e9eabb6qIiAg1adKk0udkZWWphg0bqrFjx6rMzEy1YsUKFR0drZ5++mnL7eIoofIVF7t/7L//VSosTEYJXHqpUocOKTV9uty//37n5/7jH8aIgiuvVKpZM7mdkWE85+9/N55z553lv+dbbxnPiYqSfSUlSjVubOzXo4fWrZP7zZo5f47Fi2V/585lj3/33c4jmxo0cL5/0UVKFRUp9euvSlXlq5KUJMebN8+7161ZY5zDipw/r1RgoNHuKgygUwMGGMfZtMn983Jzjec98IBsx46t/Pt6on+/DRoodcklcvv112vu/coze7ZSjRoptWVLzb7Pzp1KFRTU7Ht44+OP5XxfcYXxf0BmpnfHOH9eqfBw+Z7yv12qihoZJQQAw4YNw9y5czFjxgxcdtll+Oqrr7By5UokJCQAALKzs53mZCkpKcHEiRNx2WWXoVu3bnjllVfw7LPPYsaMGaXPiY+Px6pVq7Bp0yZ07twZ48aNw6OPPooJEyZUOSC70PlV8Bu+/nrJhDRvLqOJrroK0IO9KsqwtG8PtGolt3UWYO1a59EsH3/s3H2kmTMSp05JF82RIzLbpqYzLJ9/LtuUFOfP0aePbHftKvseOsOi21tQAFx0kXQPxcbKe/31r9L+zp0r30WkMxG7dnn3Ov3Zdu+u+C/7o0edu2/K6/6yypxhqaiORX+myEjpMgQqLvj9+mspbt66tXLt0nP69Osn2S8AqO25It96S7KENZkl+PprICkJuO463yku1hmWmBjgf/91e50t/PVXme363Dlr9VFEVVZLAVSNY4al8rKylOrY0TkT8dZbzs8pKlIqOFgee+YZpW65RW6//LI8npIi9++9V6lWreT2u+/KYydOKNWjh1IvvqjUffc5v8+xY5LpMe+791553eWXy/133nFuS0mJUjEx8tjGjc6PxcfL/lWrJEv07LNKnT4tj82Y4fw+gFI//eT9+SooMF7/hz94fv7PPxt/wT/zjPNnd2fjRud27trlfTu1qCjjOAsWuH+ezv60b6/Ujz/K7aAgpQoLy3/+vffKc66/vnLtmjjRyOJ8/73cDgtT6uzZyh3Pk59+cj7nOTlKORw1n0l67jnj/C9cWHPv4w2dDb3/fplzpzJt273b+Fxff10z7aQLQ41lWKj+iY8Hli4FAgKMfa4ZlsBAWUMIAC6/3Pir7NAhKZzUZU2TJkntDCDHBID335e/MmfOlLlUzE6cMOpXQkNle+CA/AWYkSH3Xab9gcMBXHGF3N682dhfVGTMN9OpE/CPf0g2RdfcjB4to53MXNsDyOKP48cDvXoBQ4c6Z38A58JUKxmW664DrrxS6jPMf4m6zkNj5vrXbmXrWM6dcy6CtpJhad4caN0aaNJEzqm7OW30uda1Qq527654vSmdDUtIkCHtzZvLUPQvv3R+3rvvVn1enSNHJMtx1VUycg6QEWg6y1WTtTPmqQD+7/+ca7fsUl6GxdtsoflzcDFUqg0MWAiAXOAff9y47xqwAMCbbwL//jcwcKBzl9A//yn/8V9zjRTz6oDlv/+V+VL0fCK//Wbc1o4fNy5GOjA5eBBYtUpud+3qvLij1q2bbM0By+HDknIPCZH/iF1FR0uR8bhxQO/esq+8gGX4cBn6nZ4OfPSRPHftWuCbbyQAMAcsx49XPCqqsFAuBMXFcgE3Bwz797t/nevU+JW9ILgW2ZqHLbvSj8XFOQeF331X/vP1eSgpARYtcn5s/Xrg0kuBP//Z/fvpgKVVK3m/gQPl/tq1xnPS0oBhw+SnKhYtkuDp4EEZQg9IEK3pWRF++cX5HB0+bAQ4laUnWwwJkX8Deim18+c9H3vCBPk34Bo0V5U5YLnySrn98svAK69YP4b5e8+AhWoDAxYq9dRTkj3p1MkISMzatJGLucNh/FW2Z4/UAQDAfffJtnNnqXMpLJSaBPOIIf0ftJ7n78QJY8SMnjTu8GHJygDGRcyVDlg2bZJ6mbvuMjIy+gJYnjvvlP+YO3WS+65Bwy+/SIDh5yejeXR9T79+QI8estCkazahor9MzUO/ayPDUljoHFC5ZlSsZlgA40Lmro7F/D5vvulck7NihWwrqg3Rn1F/166+WrY6oABkKDogF/3KTmatlLRPW75ctq4BS0GBfP+7dZMRMx9/LIH7M89U7n31e+sMy9ixsv3qK9kOGSLn2l0wUlgo39WMjOqvsTEHLPfcI21TSoL5L76wdgxzwFKVGisiqxiwUKmwMLk4bdvm3D1UnsRE2e7ZI3+1hofLDLSABAtDh8rtl18umy1wOOTCAMiFQhdfDhgg3TdKAZ98Ivtuu63899cBS2amBFGLF0u6HSg/2HLVurVsXTMs+q/7yy+XRSU3bgSuvda4iG/Y4HyhBioOWMzzwvz8s/P8FxVlWPTFXBcbnzolxa1z51b81/ajj8pFVgeJrvNtmLMH770nXXi6ENScYQGMgKW8DMuZM8ZFKiREgq/PPjMe18HA0aPlD6U2B1b699Wrl2w3bZKuqIICyXBpekkHb23Y4BwcLl8un/mbb4x9x4/L9/74cWnX119LNhFwzvh46+hRyTL6+RmFxZs3y/f+v/+VgH3LlvJfu3mzMdR4x47Kt6E8+nsRHS1t+3//z/g3a3GVFXYJUa1jwEJO/P3dZyfMLr0UeOIJubBHRcmFT9eKAMZ/ft9+K1t9wde39ULca9ZId0lUlOzTgRAgNQd6wjpXMTGSpVFKLp6AERyYj+GODlhcgwZ9cerXT7atWkn3lK6r2LvX6D7QrAYshw9b7xLSAUv79rI9dUpqcB57DLj4YveTrK1aJefzjTfkvn6/xo2d7//6q1xAn3nGmL3YNcOiu4T27Cmb3dDBTWioBHaABEtnz0qwYc7KlHex1UFsWJgxo3LbtvI9OHtW5gxascJ54rrt26UuKjbWuyBCZ1duv12Cq0OHJFjLyZH267omc2Zh1Srj/r591t/Lle4Oat1avs9NmkiwZp5DyN0sszoTA1Q+WHO1YQPw9tvG90B3nTocUkdUUXsOHXIe5cQuIaptDFioUhwO4MUX5cJy4oQUt5p17epcBzNsmBEkJCQAzZrJbX2xvPxyOaZ+DgCMGFFx8KQvqA0bGhdkwFqGpU0b2bpmWNaska0e1ms+ZmCgBEc6CNNZoooCFvN//vv2Oa+B5K5LSCkjYOnaVbanTkkNDCDBxkMPAR9+6Py606eN9/v4Ywkc9IVJX4yys+X4b71l/PWu64B0wKIzLDEx8rmVkskG09JkqPnJk0a3WPPmwNSp8pp9+2SBxa1bnSchKy9gMRfc6t+xwwHo+SfT042ZinUgvH271FgcOwZMnlz+uXOllJGlefhho07q0Udle8UVRvekeT7MhQuNi/CRI+UP0bdCdwe1by+f76qr5P7rrxvPcRcgmIuPqyvDcscdwL33Gt9Dc61XRcObP/5Y/hCYMsXYxy4hqm0MWKhGOBzAzTcb96++WrpWAAlkdCGtvhDoi78OWBwO+c+1InfdBTRqJP/5jxxp7LeSYdHPOXXK+M/70CHJiAQEGPUUWkAAcMklclt3kejPowOW3bulq+qRR4yFF80Zlu+/l61egeLkSeO9//UvYMwY+as/J8fILOjzsnu37HM4jELWBx90ztj88INRR6LnFnENWAoLpUspNdV43ebN8jqdNTFnw+bMkSzIV1/Jxf6662Q9Kh3cXHSRzNuil1B45hkpwjYrb4SPa/2KpgOWt9825mQZP16269cbwWJ6uvuuFLNDh+QcBwXJKDf9ndRdIoMHy4zFgHNNi2vGoLzibCt0hkUvk9Gjh2x1VlC30dX58861PPv3V89inubPFRJSfqBfXnt0N5G5Ho0ZFqptDFioxuhuIUDqE/7yF7nYjRtnZFg0fWHW/7H3729cSNy5+Wb5y+6OO2Q9Gs1KhiU83Aia9MVIdzNccYUxZblZu3ay1ZPN9esnAcTJk1IkfOutchF99VX5PAcOOAcseojvRRcZ771/vwQKDzwggdfAgVL3A8g50udABzstW8ooky5d5CIxcKAUpp47Z2RgtPfeMwKWVq0kuAMkGDBfgDdvlrbpC6LOsAByjnfulN/lxRfLvi1bjLojHdzcequ0pajICIZ0nVF5AYt5hJCZrmPZsUMu2n/6kzHqbOdO54n+rIxo0QXdHTtKhuzOO4EZM4Dnn5fA84knjAxLRessVVQgDUj3UXKyEVBpOsPiGrCYlRcgbN0qAWpkpHwPlCr7+60Mnfl6+GH5PZknZNQZlqysshPc6Tbu3WvsYw0L1bpamhemxnHiON9z/rxSY8YoNXly2cfWrnWeGE1PC372rEyXfviw9+/3xBNK3XSTUufOWXv+VVfJe7/3ntwfMULum1aNcKInOtM/+/eXXQogJkam/weUuuce50nbzEsb9OhhTK736KNlnwPI/tWrnfcNGCBt2blTpkXX+//8Z6UmTJDbetmARo3kvQClli2TCeEApZo2le3Ikcbr9UR1EREVn7PYWHlez56yfeIJ47EffzQmFwSM5RxCQsr+Tu68Ux57/nnn/WfOGEsStG6t1G+/ycR15mUK9LkLCqp48j2llJo2zfhduOP6e9UTqQFKdeok2xdeKPu6EyeU+v13uX3bbeVPQNeihexPT5f7ubnGZHUREcZyEa5efFEeu/FGpfr3l9tvvFHxZ/Xk3Dnjc506Vf7j/v7y+M8/Oz+mv0eATLinlFIJCca+hISqtY0ubJw4jmzn7w/Mmwf8/e9lHzNnWBo2NP56Dw6W+WA8ZVfK8+KLMgLE0wgnzVzHopT7+hVNZ1i05s1lcjrz6KjFi405Sf7zHyNtbp6wLjbW+LwrVhj1DLNny3mJiJAaipdeMgpSNf26Sy+V7oaJE+X+O+8Yw7pHjZIsTk6O0X0VG2tkTk6dki6SadOMc/CPf8hWF0O7o4ug9Qgbc/dRmzZSfA3I737YMKk/OXu2bIbCXMNiFhIiC1w2biwTxjVqJG3VGQoA+NvfZARTUZF8horoDIu74m3AyLAAUoR7//3GZ/vjH+W2a/uzsqT78sYb5b5enkBnngDJWukJ6XTxdESEFN8CRpfnL79IV52Z/i726WM8v6p1LOb3cJ1AEZB/N/pcuNaxmLNAOsvCLiGqbQxYyBbmyeC6dKl4zaOaYh4ptG+fFFcGBRl1FK70RQeQ0R4hIdLNsGSJdDG89550ZfXqJXUfuhCxcWMj0AAkeNDBz6JFckHv2VNGAB04IBewUaMkAHINWHQdDSAX1KefluPl5xtFo506lb2Qx8bKjzZunAQLuttGz6Vz660VnrLSC7/uMjAHLIAMLb//fuDZZyUQ1fPduHYLuathASToO3rUef2qzp1lGxgoExQ+95zcf/31imfB1Y9ZDVjatZP5UaZNk1ocfb5dA5YVK+Scr1sno7/0SCJzwKIv7NHRzrUif/mLdDtOmGAM49+zR4bnL1okdV06YLnuOuMcVnWkkLkQuryABXCewVorKHAemr53r9TgmGtqfv9dAsgLya+/SlDtGmxSzWHAQrZo0sQYHaJHwtQ2nV3Yv9+oX+nRw1giwJU5w2LORAQGykVIZ1qCgyVw0RITnS+KMTESkNx/v2QOmjYFXnhBzkfDhhLsaO4yLJqfn/FXvg4iOnaUkSC60BZwzrA0bixZCsAIWAB5bz16xh0dOGiuGZngYMnWPPmk8/PXrZML8913y2fSmYfyAhaHQwJHMx1w9Ool7bzmGsnElJRIm3WxsVISPLZuLUOg9dDxigIWczavfXvJDk2dKvMC6fPtGrCYJ3J74w3j/bOyjAu5DlhcM3MjRkjmKyHB+PwzZ0p7H35Y5iA6e1bqlZKSjAxLerpkXB5/XILrf/1LJnyzOtW/vrD6+7vPQpoLb597ToqpXbMtmZlGdiUgwPhj40IbKTR9umQRdXayNvz+u/xfVdlFW+u8WuqiqnGsYal7mjWT/u8337Tn/TdtkvcPDlbq6qvl9vTpFb9GL7o4cGDFz0tNNfr3//QnpUaNMu6/9pp37QwJMV67e3fZxz/+2Hi8cWNZHFIppb78UvZFR8u+NWuUatJEqX/+03ituZbor3/13JYdO5zrPX78seLnf/aZPM/fX6lHHnF+bWKi0VZPcnOVGjdOqa1bjX2HDhk1M1u3Sg2GrkPSv1d3NSJmJ08ar5k2zfmxo0dlv8Mh9VW//y61WY0aGa9p3tz5c2VkyGt1/cx997l/7+uvN86Pfr3+d/HQQ/Kc/HylQkPLr3Oy+ntTSqkDB+T5DRq4f85TTxl1Vvr4b7zh/H7DhhmLVcbGGjVRO3daa0d9oX93d9xRe+85bpy857//XXvvWRtYw0I+r2tX+Qutb1973j85Wf5iLSw01jhyV7+i6b+WPdV6XHedcds1w2LumrFCZ1kcjvKHbA8YIN1TgGRXdOaqTx/pJlq5Uvb94Q/yl7GecRWQcxAZKbUV5rWk3GnXzjn74dol5GrgQMmEFBcbo3pmzJAhxFu2WJukEJD2vfyyc6YkIcFYuuHjjyXT8c47kkFo3NjIKFSUXQGM7j3AuVYGkO6chg3lUq0XaJw3T+qDNNeZj3W3kO4matvW/XvrjIb5L2adMdGZs4YNJaPz6qvSdacXIQ0MlK27tZ4Aabeu0dJdQu66g8ztMR9zyRLZ6syjOcPSrJnx/bzQ6lj070nXL9UGnbXzNGqtvmLAQrZZvlxSz+bJ4mqTwwHMmmXcDw01Lgbu6HoCT3O9JCYawU1ionO3Q2UDlvh448Jq1qCBBC2ABCxmAwY414K4Bgjh4XJx+v778heZdBUYaLxH48buu8/MXn5ZAg5AhoJPmiQTqJnrOirrpptk+/HHUqgMyO9ULzAIlO3GcuVwSNdYQICxHIH5Md0tpNczGjdO7rt2z+nPqAMWfXGxErAA8r3Qk+SFhjoHzz17SnfRyJES7B08aMyJsnlz2WHI2vz50vW5cKERwJX3HdJci6ABo55G/2Gxd69R0xIVZXw/L7QuIX0OfvjBeV6dmqTP8YV2rjUGLGSbkBDPmYqa1rOnTB4GyGRxrrUTriZMkHqDMWM8H/vZZyXTctttzgFLeStJV0RfEMwFt66mTJH6DivtctW2rVHPY4UOADxlV7TmzaWQ9rbbpKi0OgusBw+W433/vWRsAgOlfueuu4zMirsiarP//lcCjfJqavR5j4qSoE7Xq4wdK9kPTS/eqSfw0wFLRb838/vddJMxcqh/f/fBoMMhr0tOlufk5zsX+5rpSQ337DECFisZFjMdDPXrJ9mrggJj9FVUlGSoAMmwbNpU+UUq6xqdYSkpqf61ntxhwEJ0gXvlFblQlDf82lWLFlKwGhXl+blDhgCffirPdS269YZ+L9e/6M2uvFK6tWqjgFkHAt4MPb/xRmDZsuoPUJs1cw5IhgyR8+XvL8sIfPCB0bVSkYgI95m+v/xFJpxbt066g7SBA42MTHCwMQQ6M1MuZrm5ElxUFAyaA4QbbpBg+JFHpAjbk4AA4/ftbkVt/Zf/mTNGl1BFGZYWLYwsnJ7ET7vkEuMc6ZlvzV1CCxbI+XBdpqM+On3aebmG6uwWOnDAeZZjMwYsRBe4hATJAHjqDqqKNm3kP/yePZ3/Krfi0ktlW94sqXYYMUJGR+jVse2mu4UAYw4VQALDoUOt18m4c8UVMiLn0kvleLNnS/asfXsjWEpKMn5PmZlGdqVly4q7zS65RB6PjJQMRnS0rJxsHkLvqW2A54Dl7FlrGZagIKO7829/cw6uW7Uyujn1+lPmDIuue6lomHl94ToyS8+BVFXvvSffpd69y9aplJQYK7VfqAGLxSm2iKgqgoJkavXKdIdMmiRT5OvhrXZr1kyG4PqKW24BnnpKLqi6lqcmmYuTR4yQczF6tASlAQHy17ceJl9RdxAgE+N99ZUEEebh7FZZDVjOnLEWsAAy/8yuXbKMRq9exiKbrVpJF9yKFcaxoqKM4l/N6jBru505I3PHREZ6/1rzvDRA9WRY3nnHuSD+m2+cs6q5uUZ35IUasDDDQlRLzHNWePu6zp3tmVyvLmjVSoLB9euNhSVrS9u2MhrogQfkwq27TN5913jck27djGJub+kuqa1b5S9yvWim5m2XECC1XKNHO6+eHRkpwdWoUcain4Bz0a3mejH3RTk5st5Xy5ZG1sIbOijThdbbt1d9bhQ9eaM+pq4T0syjsBiwEBHVUa1alb1w2kEPZ9ez0loJWKqiTRtjCPcll8h98wy0us7CapeQq0GDJBDTq5f7+cnimfpct2pldAlpubm+PeutUtJ1mJkpBcJ6RW1v6KDsyitlZFdBgTGMvbL0aumuyz1o5iDl11+NbMuFhAELEVE1mTnTOY3vqUuoqhwOGX2lHT/uPGKlvAyLNwFLx45yYf/3v419zZtLNuudd2R4enmBonmdIV8zf77RzQUYK5p7Q2dY4uKMup6qzo2i26GD3m3bnIMSc8By/nzZbJpV5iUa6hoGLERE1aRhQ7m4BwRIMKELcWvS/PnSxaHnbTFnDMqrYfHUJeQqMdHoptA6dJCh43oyQ4dDRsLpRU0r6hbatatq85a89JIUPVcmw1BUJFPqA0bNUFUClmbNjPmLqjJx3pkzxmSEAwZIJuvECee2uXYDVaYra9s26drTi6bWNQxYiIiq0RVXyJDqd98tfyK2mhAZaczSW17AUtkuISvatJEZldPSjFFFJ05IFub//s95ccANG6R43Dyayxs5OVL0PHGizC3jrQ8+AI4dk8zI7bfLvmPHvD+ODsiio6tnpl/dhuBgmUBQdyWa61hcA5bK1LGsWye/j1WrKtVM23GUEBFRNevXr/bfs7yARdewVLZLyCq92KfONhw/LvMbffutdJE9+KDs/+IL2boWlFqVlWXc/vjjsjM7e/Laa7IdPdrI0FQ1w6LnSapMN1hxsRSK6zbExUm26rLL5Pe4davRRVQdAYteyNJ8HusSZliIiOoBPXdLdXcJeUN3CZ04YdR0pKUZj3//vWxd11+y6vBh4/bHH3v32q1bZUK2gAAJoPQSGRUFLCUl5XdfVZRhMS9dUJHVqyXY+dvfjIJb3SY9OaM5sHPN4FQlYDl50nniu7qCAQsRUT2gA5b9+4Fz5+R2bXQJmekMy/79xgX2iy+MIb9btsg2J6dydSzmgOXbb40LvRVvvy3bP/1JMhk6OKioS+jWW+V5rkGNzrBERztnWH7+WaYgSEmpuMZm505pR06OFADr41cUsFRHhuXQIeP2zz8bt998U7rv3K1J5SsYsBAR1QMXXSSFpOfPS3ajuNgYXlzTXUKazrCYJ7L77TcJVI4dA44cMfZ7E2xorl0Z//mP9dfquo1bb5WtrrepKMOyerUMff7yS2OfUkYGxbVLaNcuCQy3bXO/vtPZszIBn15zaf9+IxCLi5OtDlgyM41AUwcoeq6hqmRYAONclpQYy0HoBTV9FQMWIqJ6wOFw7hYyZzDOnrU+cVxV6IDFdar6tDSjO0irTMCiL+w6k6O7hc6fl8nb3GU1jhyRIl0/P6O+yJxhKe91ublGUGGeE+X0aeNcunYJmbu6Pv20/LZkZEimo2lTWZbh/HlZgdvcprg4GZlVUmJ0rekARa8/5W3A8vvvzt1KOmA5fNjoHmLAQkREtcJceOva5aIvvrXRJaQzOwH/G9ZRXsBSmToWfZHVo4zWrZPur5kzJSvxzjvlv271atl26yYT7QFGhuXMGVnx2pW5+8ncNaOzK6GhktEyZ1jMQdjKleW3RU8q2K2bMU/PN9/IVmdYHA5jfhddk6QDFD3Pj7cBizm7Ahjn0pwJ2rDBu2PWNgYsRET1hLsMC2DM81EbGRZNL0yZng7897/Oj1UlwzJokMywW1AggZBeCkEHJq504a95rakGDYDwcLldXreQufvJnGEx168AzhkWc5fXV1+VP7nbrl2yvfRSY/iyztjoDAtg/C51QFFTAYu5SDs9vepLDNQkBixERPWElYClNjIs2oABMi/NuXNGt4euz/A2w1JSYhSKJiTIisYAsGyZrCUFOM/yqyllBDLmdZCAigtvzRmW7Gwjs2KuXwGMgOX8eedsRVGRsQimWXkBi6YzLICRYcnMlM/uGrB4O3GcDlj06uXlZVjy8ys/5Lw2MGAhIqondBfDgQP2BCyuGZbEROCjj4xFIQHghhtk623AcuyYBD5+frI8QJ8+sn/+fOM5u3cbI6ROn5b5YS6+WF7boAHQo4fzMSsa2uxa4Ksv5HqkjQ7OQkPl2IARMDVvLtvy6lh0l1B5AUt5GZYffpDuPD2Cp00b2XqbYdHt7txZtjog0wGLLub15ToWBixERPWEXogwN7fsPBu10SXUqJHzitmJiTJ66Ysv5EJ5553GhTg7W+o+Pv1UsiDFxcDw4TJHSnlFsPoC27y51MboDIs5MCsqMhYh/Pe/gTVrJHgDZAI212CtopFC5QUsq1cDf/2r3NeZIsCoY9HZl+HDZfvtt87H+PVX4706diwbsJgzVOYMiy6WbdDACIbMAcvnn7tfy+iRR4AhQ4zARJ+3rCw5z7pLSAeSDFiIiKjG6TV/zp0r22VQGxkWPz8jy+JwGEsTtGolF/x//cvo9vjlF+Chh4Drr5dunU2bgKVLgYULnYdFazqAiI+X7eWXG+sBAcbn0qOF5s2T++PHA8uXA//4R9ljWukS0utBvf22rKR85ozU0EyZYjxXByzaoEGy3bVLuoo03R2UkCD1M+aAJSoKCAoy7l98sZzD3FwjqGjSxAhKdcCyY4cEY1deCRw86NyOX38FXn1VRlN98ons69lTjnv2rGRddN2NLmT+6iv5/nz5pbRvxYqy58YuDFiIiOqJhg2N265ZA52JqMmABTAClubNy38vnSH4+WfJDAAyeZqeth8oP7jQAUTLlrINCAB69ZLbgYHG/Co7dgCbN8vw4eBgYNIkKf7Vo4PMrGRYBg+WrZ5j5aabpJsrNNR4ruuK1b16STBVWCgz32rm7iD9Ov1ac3cQIJmwxES5nZ5uPF8HLAUFEnTo0Ve//QbccovzaszmkVk6a9W2rfFeurYnOhoYOFDOx4kTwKJFwMMPS7Zq6dKy58YuDFiIiOoJPz9j5Iu7GVxrsksIMLo19MXWlc6w5OUZw4nT0oDPPjOes2RJ2RE2rhkWwKhj6dEDuOoqub19O/D663L7llvKZj/M3NWwmAt8//hHY/+wYcB775UNxMzv0bSpnONOnYz2aDrDkpRk7NNZFnPBraa7hXTBcpMmkkXT3W6//eZcNPv998CTTzrfd5WQYAR9ejK9du3kM+nXjhtntLWyyyjUBAYsRET1SGSkbN3N4FpbGRZ3AUujRmWDppwcYw6QqCgJVvRQZc01wwIAY8cCY8YAc+YYxaQbNhjzsYwZU3Fb3XUJmQt8r7gCePpp4KmngMWLJZvjypxh0Rkk8/T633wjq0zrjIbOsABGwOKaYQGMgOW772TbpIl05+hs0a+/Gt1FOhP02mtGHYoOWPT8PGFh0lYd9Omsln6fMWPkcXP9U2WGn9cUBixERPWIrmOxK2DRI5V0AOHK4XDOJpjrNuLjjb/yX3vNufhWByzmDEtkpNSqJCcbWYucHCm+/eMfjS4jd3SX0K5dQPfuwBtvOL+XLvCdNAmYPt25oNjMnGHRn01//q1bpdj4pZeMbIg5YLnmGtleeWXZ4+oC5dOnnV+nA6TsbOOYDz8MjBolt0eNkq4hvXbTiy9K8e2cOXL+XYdG6/dp2BB47DG5rQMzX8qwQNUTubm5CoDKzc21uylERLa56iqlAKX69JGt68+2bTX7/vn5Si1frlRBgfvn9OpltOfRR43b996r1PHjSoWFyf0PP5TnFxUp1aCB7Nu1y/1xW7SQ57RurdRvv3lu67FjSvn7G+/ftKm813vvyf2ePa195ldeMY4xcqTs27BB7uvjN2yo1IAB8hmLi51f/8svSpWUlD1uRoa8NiJCqblzlTp3Tvbfdpvsnz5dqaAguX3woHzmuDi5/5e/GG06edL5uMePKzV2rFLNm8vrd+40Hjt9Wqknn1Tq/feN1//+u7XzUFlWr9/MsBAR1SM6w+KuhqWmMywNG0phqrko1ZXOQkRGGsOEAZlorlkzGdkDyEic4mIpoC0okC4RnQ0ozwMPyDwlH34oXU+eREfLCJqXXpL3PXVKJnsrL5tTkfIyLDrjo2eOHTFCanXefFO6mszi4owJ3cwuu0wyNPv3A48+aix1cPXVsl28WLJJISHSVdaoETB7tjz24ouyTUgoWxTcrBnwyivyOU+fds74NGggCyEOHWqMwvKVbiEGLERE9YjdNSxW6DqP3r3lYj1ihAQiejjwk0/KxXfXLhmlomsyevUqe7E3e+opmY/EPEeKJzfcIAHSn/4k99991yjwNdfLVKS8GpbISGOhQsDorvFWly5lC4d1V5cegdS2rXFebrtN5njR3Wldu7o/tp+fEQS5Mnfd+Uq3EAMWIqJ6RGdYcnPLf7ymRwlZMXy4FHrqTMqiRbKasi4mbdQI+Mtf5Pbs2UbAoic9qwm33Sbbjz4y5iwxBxwVMQcUOmABjMCpa9eKAwdvde7sPIRdF80CUmczdapxPzm58u+jPwszLEREVO10wOKOL2RYrrpKRrf07+/+OaNHS1szMowp7nVXSE3o00e6iH79VWbHTUgwghhPyusSAmQYdHCw8yRz1SEgwBjGDTgHLIAM59bBUt++lX8fHbAww0JERNXOU8DiCxkWK5o2NSaDKyqSmpiqZAs88feXCz0gNR6rVlU8h4uZuUvIHLAMHy4T9g0ZUm3NLGUO3lzrevz8pF5m7dqqBXn6s/hKhsVN7xUREdVFuoZFi4iQSdo0X8iwWDVmjEznD8iwY/MQ6JowZYoM5x01quw6PxVp0AC45x4ZJuxa91JeMW11MA/Zds2wABJ06SHTlWXOsMyZIxPqDRtmvRi5ujFgISKqR1wzLE2aGAFLQEDFRau+pmdPGW2zc2fNdgdpsbHA3LmVe+3bb1dnSzy76ipjVuPyApbqYC66XbtWtpddxoCFiIiqQXkBy6FDcruudAdpDodMs//yyzIxGhkaNpSFCktKjMCluukMy7ffyrDykJDaCRzdYcBCRFSPuHYJ6cXygLrVHaT17Ck/VNZll9Xs8XWGRU/V36ePvUFvHUoOEhGRJ+VlWLS6GLCQfcxDtAEgJcWedmgMWIiI6hHXgEXPbQLUvS4hsld4uBQUa9dea19bAAYsRET1CjMsVF0cDiPLEhMDdOpkb3sqFbCkpqYiMTERISEhSE5Oxno9DWE5NmzYgF69eqFp06YIDQ1F+/bt8dJLL7l9/tKlS+FwODCkJgauExHVc641LOY5QhiwkLd0wHLttTU3RNsqr4tuly1bhvHjxyM1NRW9evXC66+/jkGDBmH37t1oWc7CC2FhYRg7diw6d+6MsLAwbNiwAaNHj0ZYWBgefPBBp+f+9NNPePLJJ9G7JudfJiKqx8LC5MKi15JhlxBVRY8eMhpp+HC7WwI4lNJfa2u6d++Orl27Yt68eaX7OnTogCFDhmDWrFmWjjF06FCEhYXhnXfeKd1XXFyMvn374t5778X69euRk5OD5cuXW25XXl4eIiMjkZubiwhPUz0SEdVjkZHG3CsffQTcfLPc7tsXWLfOtmZRHVRUJDPdJiTU3HtYvX571SVUVFSELVu2IMWlVDglJQXp6emWjpGRkYH09HT0dVngYMaMGWjWrBnuv/9+b5pEREQuzP/ns4aFqiIoqGaDFW941SV08uRJFBcXIyYmxml/TEwMjrpby/x/WrRogRMnTuD8+fOYNm0aRpnW2t64cSPeeOMNbN261XJbCgsLUVhYWHo/zzz3NBHRBSwyEvj5Z7ltDljYJUR1WaWKbh0ulTdKqTL7XK1fvx6bN2/G/PnzMXfuXCxZsgQAkJ+fj7vuugsLFy5ElNWVpgDMmjULkZGRpT/xds0VTETkY3SGxeEAGjUy9jPDQnWZVxmWqKgo+Pv7l8mmHD9+vEzWxVViYiIAoFOnTjh27BimTZuG4cOHY//+/Th06BAGDx5c+tySkhJpXEAAMjMz0aZNmzLHmzhxIh5//PHS+3l5eQxaiIhgBCwhIbLKscYMC9VlXgUsQUFBSE5ORlpaGm7WVVwA0tLScNNNN1k+jlKqtDunffv22LFjh9PjkydPRn5+Pl5++WW3QUhwcDCC+ecCEVEZOmBp0MA5SOF/mVSXeT2s+fHHH8eIESPQrVs39OjRAwsWLEBWVhbGjBkDQDIfR44cwaJFiwAAr732Glq2bIn27dsDkHlZXnzxRTzyyCMAgJCQECQlJTm9R6P/5TBd9xMRkWd6LpbQUOcMCwMWqsu8DliGDRuGU6dOYcaMGcjOzkZSUhJWrlyJhP+VEWdnZyMrK6v0+SUlJZg4cSIOHjyIgIAAtGnTBs8++yxGjx5dfZ+CiIhK6QxLaCjg5ycjPYqK2CVEdZvX87D4Ks7DQkQkpk8Hpk0DunQBtm415mX529+AmTPtbh2RsxqZh4WIiHyfuUvIvGWXENVlDFiIiOoZc5eQecsuIarLGLAQEdUz/foBHTsa67/oQIUZFqrLvC66JSIi39aqFbBrl3GfXUJUHzDDQkRUz+kMC7uEqC5jwEJEVM9FR8vWi9VPiHwOu4SIiOq5F14ABg0CBg60uyVElceAhYionrvkEvkhqsvYJUREREQ+jwELERER+TwGLEREROTzGLAQERGRz2PAQkRERD6PAQsRERH5PAYsRERE5PMYsBAREZHPY8BCREREPo8BCxEREfk8BixERETk8xiwEBERkc9jwEJEREQ+r96s1qyUAgDk5eXZ3BIiIiKySl+39XXcnXoTsOTn5wMA4uPjbW4JEREReSs/Px+RkZFuH3coTyFNHVFSUoJffvkF4eHhcDgc1XbcvLw8xMfH4/Dhw4iIiKi241JZPNe1g+e59vBc1w6e59pTE+daKYX8/Hw0b94cfn7uK1XqTYbFz88PLVq0qLHjR0RE8B9CLeG5rh08z7WH57p28DzXnuo+1xVlVjQW3RIREZHPY8BCREREPo8BiwfBwcGYOnUqgoOD7W5KvcdzXTt4nmsPz3Xt4HmuPXae63pTdEtERET1FzMsRERE5PMYsBAREZHPY8BCREREPo8BCxEREfk8BiwepKamIjExESEhIUhOTsb69evtblKdNm3aNDgcDqef2NjY0seVUpg2bRqaN2+O0NBQXHPNNdi1a5eNLa47vvrqKwwePBjNmzeHw+HA8uXLnR63cm4LCwvxyCOPICoqCmFhYfjjH/+In3/+uRY/he/zdJ5HjhxZ5jt+1VVXOT2H59mzWbNm4YorrkB4eDiio6MxZMgQZGZmOj2H3+nqYeVc+8L3mgFLBZYtW4bx48dj0qRJyMjIQO/evTFo0CBkZWXZ3bQ67dJLL0V2dnbpz44dO0ofe/755zFnzhy8+uqr2LRpE2JjY3HttdeWrhVF7p0+fRpdunTBq6++Wu7jVs7t+PHj8dFHH2Hp0qXYsGEDfv/9d9x4440oLi6urY/h8zydZwC47rrrnL7jK1eudHqc59mzL7/8Eg8//DC++eYbpKWl4fz580hJScHp06dLn8PvdPWwcq4BH/heK3LryiuvVGPGjHHa1759ezVhwgSbWlT3TZ06VXXp0qXcx0pKSlRsbKx69tlnS/edPXtWRUZGqvnz59dSC+sHAOqjjz4qvW/l3Obk5KjAwEC1dOnS0uccOXJE+fn5qc8++6zW2l6XuJ5npZS655571E033eT2NTzPlXP8+HEFQH355ZdKKX6na5LruVbKN77XzLC4UVRUhC1btiAlJcVpf0pKCtLT021qVf2wb98+NG/eHImJibj99ttx4MABAMDBgwdx9OhRp3MeHByMvn378pxXkZVzu2XLFpw7d87pOc2bN0dSUhLPv5fWrVuH6OhotG3bFg888ACOHz9e+hjPc+Xk5uYCAJo0aQKA3+ma5HquNbu/1wxY3Dh58iSKi4sRExPjtD8mJgZHjx61qVV1X/fu3bFo0SJ8/vnnWLhwIY4ePYqePXvi1KlTpeeV57z6WTm3R48eRVBQEBo3buz2OeTZoEGDsHjxYqxZswazZ8/Gpk2b0K9fPxQWFgLgea4MpRQef/xxXH311UhKSgLA73RNKe9cA77xva43qzXXFIfD4XRfKVVmH1k3aNCg0tudOnVCjx490KZNG/zzn/8sLeDiOa85lTm3PP/eGTZsWOntpKQkdOvWDQkJCfjvf/+LoUOHun0dz7N7Y8eOxfbt27Fhw4Yyj/E7Xb3cnWtf+F4zw+JGVFQU/P39y0SGx48fLxPRU+WFhYWhU6dO2LdvX+loIZ7z6mfl3MbGxqKoqAi//fab2+eQ9+Li4pCQkIB9+/YB4Hn21iOPPIJPPvkEa9euRYsWLUr38ztd/dyd6/LY8b1mwOJGUFAQkpOTkZaW5rQ/LS0NPXv2tKlV9U9hYSH27NmDuLg4JCYmIjY21umcFxUV4csvv+Q5ryIr5zY5ORmBgYFOz8nOzsbOnTt5/qvg1KlTOHz4MOLi4gDwPFullMLYsWPx4YcfYs2aNUhMTHR6nN/p6uPpXJfHlu91tZTu1lNLly5VgYGB6o033lC7d+9W48ePV2FhYerQoUN2N63OeuKJJ9S6devUgQMH1DfffKNuvPFGFR4eXnpOn332WRUZGak+/PBDtWPHDjV8+HAVFxen8vLybG6578vPz1cZGRkqIyNDAVBz5sxRGRkZ6qefflJKWTu3Y8aMUS1atFCrV69W33//verXr5/q0qWLOn/+vF0fy+dUdJ7z8/PVE088odLT09XBgwfV2rVrVY8ePdRFF13E8+ylhx56SEVGRqp169ap7Ozs0p+CgoLS5/A7XT08nWtf+V4zYPHgtddeUwkJCSooKEh17drVaZgXeW/YsGEqLi5OBQYGqubNm6uhQ4eqXbt2lT5eUlKipk6dqmJjY1VwcLDq06eP2rFjh40trjvWrl2rAJT5ueeee5RS1s7tmTNn1NixY1WTJk1UaGiouvHGG1VWVpYNn8Z3VXSeCwoKVEpKimrWrJkKDAxULVu2VPfcc0+Zc8jz7Fl55xiAeuutt0qfw+909fB0rn3le+34X2OJiIiIfBZrWIiIiMjnMWAhIiIin8eAhYiIiHweAxYiIiLyeQxYiIiIyOcxYCEiIiKfx4CFiIiIfB4DFiIiIvJ5DFiIiIjI5zFgISIiIp/HgIWIiIh8HgMWIiIi8nn/HwrbbYx9oaEmAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Vamos ver como foi o treino?\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "legend = plt.legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 2ms/step\n",
      "train loss, test acc: 0.34235572814941406\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "test loss, test acc: 0.40339386463165283\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "results_train = model.evaluate(X_train, y_train, verbose = 0)\n",
    "y_hat_train = model.predict(X_train)\n",
    "y_hat_train = [np.max(y) for y in y_hat_train]\n",
    "error_train = y_hat_train - y_train\n",
    "print('train loss, test acc:', results_train)\n",
    "\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test, verbose = 0)\n",
    "y_hat_test = model.predict(X_test)\n",
    "y_hat_test = [np.max(y) for y in y_hat_test]\n",
    "error_test = y_hat_test - y_test\n",
    "\n",
    "print('test loss, test acc:', results_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement ICoNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train df\n",
    "X_train_V = X_train.copy()  \n",
    "X_train_V['error'] = error_train\n",
    "X_train_V['Survived'] = y_train\n",
    "\n",
    "\n",
    "# Test df\n",
    "X_test_V = X_test.copy()\n",
    "X_test_V['error'] = error_test\n",
    "X_test_V['Survived'] = y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoder_train = X_train_V.copy()\n",
    "df_outs_train = df_encoder_train['Survived']\n",
    "df_features_train = df_encoder_train.drop(['error','Survived'], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "df_encoder_test = X_test_V.copy()\n",
    "df_outs_test = df_encoder_test['Survived']\n",
    "df_features_test = df_encoder_test.drop(['error','Survived'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 1\n",
    "number_of_features = len(df_encoder_train.columns)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Iconet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "from numpy.random import seed\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from keras.layers import Input, Dense, concatenate\n",
    "from keras.models import Model\n",
    "from tensorflow.python.client import device_lib\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keras.backend.clear_session()\n",
    "input_dim = Input(shape = (number_of_features, ))\n",
    "input_dim2 = Input(shape = (number_of_features - 2, ))\n",
    "# Encoder Layers\n",
    "model.add(BatchNormalization())\n",
    "encoded1 = Dense(40, activation = 'relu')(input_dim)\n",
    "model.add(BatchNormalization())\n",
    "encoded2 = Dense(20, activation = 'relu')(encoded1)\n",
    "model.add(BatchNormalization())\n",
    "encoded5 = Dense(5, activation = 'relu')(encoded2)\n",
    "encoded13 = Dense(encoding_dim, activation = 'linear')(encoded5)\n",
    "merged = keras.layers.concatenate([encoded13, input_dim2], axis=-1)\n",
    "# Decoder Layers\n",
    "model.add(BatchNormalization())\n",
    "decoded1 = Dense(100, activation = 'relu')(merged)\n",
    "model.add(BatchNormalization())\n",
    "decoded2 = Dense(500, activation = 'relu')(decoded1)\n",
    "model.add(BatchNormalization())\n",
    "decoded3 = Dense(500, activation = 'relu')(decoded2)\n",
    "model.add(BatchNormalization())\n",
    "decoded4 = Dense(250, activation = 'relu')(decoded3)\n",
    "model.add(BatchNormalization())\n",
    "decoded5 = Dense(50, activation = 'relu')(decoded4)\n",
    "\n",
    "\n",
    "decoded13 = Dense(2, activation = 'sigmoid')(decoded4)\n",
    "\n",
    "# Combine Encoder and Deocder layers\n",
    "counter = Model(inputs = [input_dim, input_dim2], outputs = [decoded13])\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate= 0.0001)\n",
    "# Compile the Model\n",
    "counter.compile(optimizer = 'adam', loss = 'mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "5/5 [==============================] - 2s 73ms/step - loss: 0.3932 - val_loss: 0.4067\n",
      "Epoch 2/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3726 - val_loss: 0.4068\n",
      "Epoch 3/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 4/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 5/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 6/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 7/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 8/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 9/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 10/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 11/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 12/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 13/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 14/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 15/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 16/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 17/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 18/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 19/500\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 20/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 21/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 22/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 23/500\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 24/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 25/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 26/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 27/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 28/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 29/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 30/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 31/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 32/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 33/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 34/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 35/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 36/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 37/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 38/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 39/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 40/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 41/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 42/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 43/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 44/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 45/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 46/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 47/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 48/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 49/500\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 50/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 51/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 52/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 53/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 54/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 55/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 56/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 57/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 58/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 59/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 60/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 61/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 62/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 63/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 64/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 65/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 66/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 67/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 68/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 69/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 70/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 71/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 72/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 73/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 74/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 75/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 76/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 77/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 78/500\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 79/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 80/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 81/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 82/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 83/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 84/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 85/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 86/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 87/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 88/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 89/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 90/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 91/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 92/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 93/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 94/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 95/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 96/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 97/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 98/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 99/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 100/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 101/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 102/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 103/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 104/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 105/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 106/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 107/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 108/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 109/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 110/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 111/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 112/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 113/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 114/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 115/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 116/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 117/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 118/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 119/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 120/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 121/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 122/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 123/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 124/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 125/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 126/500\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 127/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 128/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 129/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 130/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 131/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 132/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 133/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 134/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 135/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 136/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 137/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 138/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 139/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 140/500\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 141/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 142/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 143/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 144/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 145/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 146/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 147/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 148/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 149/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 150/500\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 151/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 152/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 153/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 154/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 155/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 156/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 157/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 158/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 159/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 160/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 161/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 162/500\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 163/500\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 164/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 165/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 166/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 167/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 168/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 169/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 170/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 171/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 172/500\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 173/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 174/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 175/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 176/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 177/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 178/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 179/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 180/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 181/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 182/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 183/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 184/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 185/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 186/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 187/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 188/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 189/500\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 190/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 191/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 192/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 193/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 194/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 195/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 196/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 197/500\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 198/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 199/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 200/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 201/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 202/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 203/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 204/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 205/500\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 206/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 207/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 208/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 209/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 210/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 211/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 212/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 213/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 214/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 215/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 216/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 217/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 218/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 219/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 220/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 221/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 222/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 223/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 224/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 225/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 226/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 227/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 228/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 229/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 230/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 231/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 232/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 233/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 234/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 235/500\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 236/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 237/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 238/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 239/500\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 240/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 241/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 242/500\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 243/500\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 244/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 245/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 246/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 247/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 248/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 249/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 250/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 251/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 252/500\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 253/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 254/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 255/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 256/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 257/500\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 258/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 259/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 260/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 261/500\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 262/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 263/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 264/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 265/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 266/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 267/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 268/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 269/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 270/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 271/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 272/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 273/500\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 274/500\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 275/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 276/500\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 277/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 278/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 279/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 280/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 281/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 282/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 283/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 284/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 285/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 286/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 287/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 288/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 289/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 290/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 291/500\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 292/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 293/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 294/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 295/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 296/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 297/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 298/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 299/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 300/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 301/500\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 302/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 303/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 304/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 305/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 306/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 307/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 308/500\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 309/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 310/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 311/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 312/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 313/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 314/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 315/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 316/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 317/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 318/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 319/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 320/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 321/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 322/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 323/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 324/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 325/500\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 326/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 327/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 328/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 329/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 330/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 331/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 332/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 333/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 334/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 335/500\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 336/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 337/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 338/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 339/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 340/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 341/500\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 342/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 343/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 344/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 345/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 346/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 347/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 348/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 349/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 350/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 351/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 352/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 353/500\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 354/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 355/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 356/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 357/500\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 358/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 359/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 360/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 361/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 362/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 363/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 364/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 365/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 366/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 367/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 368/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 369/500\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 370/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 371/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 372/500\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 373/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 374/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 375/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 376/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 377/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 378/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 379/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 380/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 381/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 382/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 383/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 384/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 385/500\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 386/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 387/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 388/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 389/500\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 390/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 391/500\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 392/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 393/500\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 394/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 395/500\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 396/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 397/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 398/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 399/500\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 400/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 401/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 402/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 403/500\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 404/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 405/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 406/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 407/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 408/500\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 409/500\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 410/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 411/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 412/500\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 413/500\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 414/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 415/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 416/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 417/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 418/500\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 419/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 420/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 421/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 422/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 423/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 424/500\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 425/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 426/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 427/500\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 428/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 429/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 430/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 431/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 432/500\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 433/500\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 434/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 435/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 436/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 437/500\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 438/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 439/500\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 440/500\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 441/500\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 442/500\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 443/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 444/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 445/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 446/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 447/500\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 448/500\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 449/500\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 450/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 451/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 452/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 453/500\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 454/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 455/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 456/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 457/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 458/500\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 459/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 460/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 461/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 462/500\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 463/500\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 464/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 465/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 466/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 467/500\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 468/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 469/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 470/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 471/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 472/500\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 473/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 474/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 475/500\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 476/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 477/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 478/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 479/500\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 480/500\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 481/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 482/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 483/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 484/500\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 485/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 486/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 487/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 488/500\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 489/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 490/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 491/500\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 492/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 493/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 494/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 495/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 496/500\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 497/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 498/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 499/500\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.3725 - val_loss: 0.4068\n",
      "Epoch 500/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3725 - val_loss: 0.4068\n"
     ]
    }
   ],
   "source": [
    "es = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=1000,\n",
    "                              verbose=0, mode='auto',\n",
    "                                  restore_best_weights=True)\n",
    "\n",
    "\n",
    "callbacks = [es]\n",
    "\n",
    "history = counter.fit(x = [df_encoder_train, df_features_train], y = df_outs_train, epochs = 500, \n",
    "                batch_size = 128, shuffle = True,\n",
    "                validation_data=([df_encoder_test,df_features_test],df_outs_test),\n",
    "               #callbacks = callbacks\n",
    "                \n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Vamos ver como foi o treino?\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "legend = plt.legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "counter = Model(inputs = [input_dim,input_dim2], outputs = decoded13)\n",
    "encoded_input = Input(shape = (encoding_dim, ))\n",
    "\n",
    "encoded_test = pd.DataFrame(counter.predict([df_encoder_test,df_features_test]))\n",
    "encoded_test = encoded_test.add_prefix('feature_')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Survived'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\PLour\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3628\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3629\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\PLour\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\PLour\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Survived'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1632\\1214130138.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'feature_0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_outs_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Survived'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\PLour\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 958\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\PLour\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1069\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1070\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\PLour\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3629\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3631\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3632\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3633\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Survived'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "mean_absolute_error(encoded_test['feature_0'], df_outs_test['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Survived'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\PLour\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3628\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3629\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\PLour\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\PLour\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Survived'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1632\\963331714.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'feature_0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_outs_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Survived'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\PLour\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 958\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\PLour\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1069\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1070\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\PLour\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3629\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3631\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3632\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3633\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Survived'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(encoded_test['feature_0'], df_outs_test['Survived'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
